## 乱七八糟的小知识点

有时候觉得学ds真无语，啥都要知道一点；又要懂编程又要懂spectral theorem的proof。每回准备面试都要把两年前做的project重新挖出来看看有什么忘了的。干脆趁现在把乱七八糟的知识点总结一下

<h2>
    To find the association between X and Y <br /> 
</h2>
```
if X abd Y are both numeric
  to determne the strength of relationship
    if at least one variable is normally distributed
      Person's correlation coefficient
    if none is normally distributed
      Spearman's correlaton coefficient
  to predict one using the other(s)
    simple linear regression or multiple linear regression
if only Y is categorical
  if class(Y)= 2
    simple logistic regression
  if class(Y) >=3
    Y is nominal
      multinomial logistic regression
    Y is ordinal
      ordered logistic regression
```
<h3>
    for comparing the difference between mean, continuous variable and normally distributed <br /> 
</h3>

```
if comparing sample mean to "true population mean"
    &sigma; is known
        z test
    &sigma; is not known
        t test, but when n>= 30, use z test
if comparing mean between 2 samples
    if it is a paired matched sample
        paired t test
    if independent samples
        &sigma; is the same
            two sample t-test
        &sigma; is not the same
            welch t-test
if comparing mean between >= 3 groups
    paired samples
        RBD
    independent
        ANOVA (use MANOVA when more than one vairable is used to calculate the mean)
```
<h4>
    other random test and notes: <br /> 
</h4>
<b> Randomization test</b> : used to compare mean between two groups. Can be used in conjunction with traditional t test <br /> 
<b> Fisher’s Exact Test</b> : Fisher's exact test is a statistical test used to determine if there are nonrandom associations between two categorical variables. <br /> 
<b> Non-parametric bootstrap test</b> : use the existing data <br /> 
<b> Parametric bootstrap test</b> :  assumption is normal distribution, creates "new data" from existing sample mean and variance <br /> 
<b> Checking normality</b> : use histogram or qq plot <br /> 
<b> LDA </b> : a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. Under LDA, the assumptions are that classes are well separated. LDA assumes that data is drawn from a multivariable Gaussian set: assuming each class has its own mean and all share a covariance matrix signma. <br /> 
<b> QDA </b> : Quadratic discriminant analysis (QDA) is closely related to linear discriminant analysis (LDA), where it is assumed that the measurements from each class are normally distributed. Unlike LDA however, in QDA there is no assumption that the covariance of each of the classes is identical. QDA assumes that each class is normal, and assumes each class has its own coveriance matrix. <br /> 
<h5>
    validation stuff: <br /> 
</h5>
<b> VIF</b> : used to identify multicolinearity. Similarily we can also just use a correlation plot <br /> 
<b> powr transformation </b> : make a non-normal distribution into a approximately normal one. boxcox() in R. <br /> 
<b> AIC model selection </b> : Derived from frequentist probability, penalizes complex models less, meaning that it may put more emphasis on model performance on the training dataset, and, in turn, select more complex models. Use AICc when the data size is small. <br /> 
<b> BIC model selection </b> : Derived from Bayesian probability, penalizes the model more for its complexity. Importantly, the derivation of BIC under the Bayesian probability framework means that if a selection of candidate models includes a true model for the dataset, then the probability that BIC will select the true model increases with the size of the training dataset <br /> 
<b> Cook's distance</b> : can be used to identify outliers <br /> 
<b> LOOCV, k-FOLD, Valiation Step </b> <br /> 
<p> LOOCV (Leave one out cross validation): divide the data set into two parts,
in one part we have one single observation, which is our test data. Repeat the
process n times, which generates n times MSE. There is far less bias for this
method and no randomness in the testing/training data set will yield the same
results. But MSE will vary as tset data uses a single observation (what if the
data point is an outlier?) and it takes forever to run this too </p>
<p> K fold cross validation: divide the dataset into k groups of approximately equal
size. the first fold is kept for testing and the model is trained on k-1 folds. Repeat
the process for K times and each time different fold are used for validation.
Cross Validation error is computed by taking the average of the MSE over k
folds. LOOCV is k=n. K fold reduces bias, reduce computation time, and as k
increases, variance of the resulting estimate also decrease, and every data points
get to be test exactly once an is used in training k-1 times. But again this might
also ask for too much computational power. <br /> 
The lower the k, the higher the bias and lower the variance. The higher the k,
the lower the bias but higher the variance. </p>

<p> Step approach: randomly dividing the dataset into two parts. Validation test error rate is highly variable depending on which observation are in
the training and validation set. Also, only a small subset of the observations are used to fit the model. MSE varies greatly.But yes, it is easy to run.  </p>

<h5>
    Other random things: <br /> 
</h5>
<b> Principle Component Analysis </b> : k principal components can replace the initial p variables, can be used for data reduction and interpretation. Want to standardize varibles first. Uses eigenvectors and eigenvalues. <br/>
<b> Hotelling T-test</b>: used when the number of response variables are two or more <br/>
<b> chi-square plot </b>: check bivariate normality <br/>
<b> EM algorithm </b> expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. <br/>
<b> Chi-Square Independence Test </b> : statistical hypothesis test used to determine whether two categorical or nominal variables are likely to be related or not.<br/>

<b> I refuse to write about different models because I refuse to believe I need to recite everthing I know about them on the spot. </b>
